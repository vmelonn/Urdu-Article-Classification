{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [],
   "source": [
    "# %pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "\n",
    "    def get_express_articles(self, max_pages=7):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "\n",
    "    def get_geo_article(self):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = \"https://urdu.geo.tv/\"\n",
    "        categories = [\n",
    "            \"entertainment\",\n",
    "            \"business\",\n",
    "            \"sports\",\n",
    "            \"science-technology\",\n",
    "            \"world\",\n",
    "        ]\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/category/{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Finding article cards\n",
    "            cards = soup.find_all(\n",
    "                \"div\", class_=\"col-xs-6 col-sm-6 col-lg-6 col-md-6 singleBlock\"\n",
    "            )  # Adjust class as per actual site structure\n",
    "            print(f\"\\t--> Found {len(cards)} articles of '{category}'.\")\n",
    "            success_count = 0\n",
    "\n",
    "            for card in cards:\n",
    "                try:\n",
    "                    # Locate the nested <a> tag\n",
    "                    link_tag = card.find(\"a\", class_=\"open-section\")\n",
    "\n",
    "                    # Extract the title from the 'title' attribute of the <a> tag\n",
    "                    headline = link_tag[\"title\"].strip()\n",
    "                    # Extract the href (link) from the <a> tag\n",
    "                    link = link_tag[\"href\"].strip()\n",
    "\n",
    "                    # Requesting the content from each article's link\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                    # Content is arranged in paras inside <div class='storyDetail'>\n",
    "                    paras = content_soup.find(\"div\", class_=\"content-area\").find_all(\n",
    "                        \"p\"\n",
    "                    )\n",
    "                    combined_text = \" \".join(\n",
    "                        p.get_text(strip=True)\n",
    "                        .replace(\"\\xa0\", \" \")\n",
    "                        .replace(\"\\u200b\", \"\")\n",
    "                        for p in paras\n",
    "                        if p.get_text(strip=True)\n",
    "                    )\n",
    "\n",
    "                    # Store the scraped data\n",
    "                    geo_df[\"id\"].append(self.id)\n",
    "                    geo_df[\"title\"].append(headline)\n",
    "                    geo_df[\"link\"].append(link)\n",
    "                    geo_df[\"gold_label\"].append(category)\n",
    "                    geo_df[\"content\"].append(combined_text)\n",
    "\n",
    "                    # Increment ID and success count\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape an article of '{category}': {e}\")\n",
    "\n",
    "            print(\n",
    "                f\"\\t--> Successfully scraped {success_count} articles from'{category}'.\"\n",
    "            )\n",
    "        return pd.DataFrame(geo_df)\n",
    "    \n",
    "    def get_jang_article(self):\n",
    "        jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = \"https://jang.com.pk/category/latest-news/\"\n",
    "        categories = [\n",
    "            \"sports\",\n",
    "            \"business\",\n",
    "            \"entertainment\",\n",
    "            \"world\",\n",
    "            \"science-technology\",\n",
    "        ]\n",
    "\n",
    "        # Iterating over the specified categories\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}{category}/\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Finding article list items\n",
    "            articles = soup.find(\"ul\", class_=\"scrollPaginationNew__\").find_all(\"li\")\n",
    "            print(f\"\\t--> Found {len(articles)} articles in '{category}' category.\")\n",
    "            success_count = 0\n",
    "\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    # Locate the nested <a> tag within the article\n",
    "                    link_tag = article.find(\"a\")\n",
    "                    if not link_tag:\n",
    "                        continue\n",
    "\n",
    "                    # Extract the href (link) from the <a> tag\n",
    "                    link = link_tag[\"href\"].strip()\n",
    "\n",
    "                    #article_id = link.split(\"/\")[-1]\n",
    "                    # Requesting the content from each article's link\n",
    "                    main_heading = article.find(\"div\", class_=\"main-heading\")\n",
    "                    title_tag = main_heading.find(\"h2\") if main_heading else None\n",
    "                    headline = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                    # Extracting content from <div class='main-heading'> in the article\n",
    "                    content_div = content_soup.find(\"div\", class_=\"detail_view_content\")\n",
    "                    if content_div:\n",
    "                        paras = content_div.find_all(\"p\")\n",
    "                        combined_text = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in paras\n",
    "                            if p.get_text(strip=True)\n",
    "                        )\n",
    "                    else:\n",
    "                        combined_text = \"No Content\"\n",
    "                    # Store the scraped data\n",
    "                    jang_df[\"id\"].append(self.id)\n",
    "                    jang_df[\"title\"].append(headline)\n",
    "                    jang_df[\"link\"].append(link)\n",
    "                    jang_df[\"gold_label\"].append(category)\n",
    "                    jang_df[\"content\"].append(combined_text)\n",
    "\n",
    "                    # Increment ID and success count\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape an article of '{category}': {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "        return pd.DataFrame(jang_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 1 of 'saqafat'.\n",
      "\t--> Failed to scrape an article on page 1 of 'saqafat': 404 Client Error: Not Found for url: https://www.express.pk/story/2735209/akshaykumar-ki-film-housefull5-ki-shooting-iktitami-marahil-main-dakhil-2735209\n",
      "\t--> Failed to scrape an article on page 1 of 'saqafat': 404 Client Error: Not Found for url: https://www.express.pk/story/2735208/amirkhan-ke-mashware-par-lahore1947-main-emotional-seens-shamil-2735208\n",
      "\t--> Failed to scrape an article on page 1 of 'saqafat': 404 Client Error: Not Found for url: https://www.express.pk/story/2735207/rashmika-mandana-ka-film-pushpa2-ke-set-ko-emotional-alwida-2735207\n",
      "\t--> Failed to scrape an article on page 1 of 'saqafat': 404 Client Error: Not Found for url: https://www.express.pk/story/2735196/napa-ka-bara-elan-radiodrama-ki-riwayat-ko-zinda-kia-jayega-2735196\n",
      "\t--> Successfully scraped 6 articles from page 1 of 'saqafat'.\n",
      "Scraping page 2 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 2 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'saqafat'.\n",
      "Scraping page 3 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 3 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'saqafat'.\n",
      "Scraping page 4 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 4 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'saqafat'.\n",
      "Scraping page 5 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 5 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'saqafat'.\n",
      "Scraping page 6 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 6 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'saqafat'.\n",
      "Scraping page 7 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 7 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'saqafat'.\n",
      "\n",
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 10 articles on page 1 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'business'.\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 10 articles on page 2 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'business'.\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 10 articles on page 3 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'business'.\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 10 articles on page 4 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'business'.\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 10 articles on page 5 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'business'.\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 10 articles on page 6 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'business'.\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 10 articles on page 7 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'business'.\n",
      "\n",
      "Scraping page 1 of category 'sports'...\n",
      "\t--> Found 10 articles on page 1 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'sports'.\n",
      "Scraping page 2 of category 'sports'...\n",
      "\t--> Found 10 articles on page 2 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'sports'.\n",
      "Scraping page 3 of category 'sports'...\n",
      "\t--> Found 10 articles on page 3 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'sports'.\n",
      "Scraping page 4 of category 'sports'...\n",
      "\t--> Found 10 articles on page 4 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'sports'.\n",
      "Scraping page 5 of category 'sports'...\n",
      "\t--> Found 10 articles on page 5 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'sports'.\n",
      "Scraping page 6 of category 'sports'...\n",
      "\t--> Found 10 articles on page 6 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'sports'.\n",
      "Scraping page 7 of category 'sports'...\n",
      "\t--> Found 10 articles on page 7 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'sports'.\n",
      "\n",
      "Scraping page 1 of category 'science'...\n",
      "\t--> Found 10 articles on page 1 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'science'.\n",
      "Scraping page 2 of category 'science'...\n",
      "\t--> Found 10 articles on page 2 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'science'.\n",
      "Scraping page 3 of category 'science'...\n",
      "\t--> Found 10 articles on page 3 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'science'.\n",
      "Scraping page 4 of category 'science'...\n",
      "\t--> Found 10 articles on page 4 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'science'.\n",
      "Scraping page 5 of category 'science'...\n",
      "\t--> Found 10 articles on page 5 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'science'.\n",
      "Scraping page 6 of category 'science'...\n",
      "\t--> Found 10 articles on page 6 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'science'.\n",
      "Scraping page 7 of category 'science'...\n",
      "\t--> Found 10 articles on page 7 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'science'.\n",
      "\n",
      "Scraping page 1 of category 'world'...\n",
      "\t--> Found 10 articles on page 1 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'world'.\n",
      "Scraping page 2 of category 'world'...\n",
      "\t--> Found 10 articles on page 2 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'world'.\n",
      "Scraping page 3 of category 'world'...\n",
      "\t--> Found 10 articles on page 3 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'world'.\n",
      "Scraping page 4 of category 'world'...\n",
      "\t--> Found 10 articles on page 4 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'world'.\n",
      "Scraping page 5 of category 'world'...\n",
      "\t--> Found 10 articles on page 5 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'world'.\n",
      "Scraping page 6 of category 'world'...\n",
      "\t--> Found 10 articles on page 6 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'world'.\n",
      "Scraping page 7 of category 'world'...\n",
      "\t--> Found 10 articles on page 7 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'world'.\n",
      "\n",
      "Scraping category 'entertainment'...\n",
      "\t--> Found 60 articles of 'entertainment'.\n",
      "\t--> Successfully scraped 60 articles from'entertainment'.\n",
      "Scraping category 'business'...\n",
      "\t--> Found 60 articles of 'business'.\n",
      "\t--> Successfully scraped 60 articles from'business'.\n",
      "Scraping category 'sports'...\n",
      "\t--> Found 60 articles of 'sports'.\n",
      "\t--> Successfully scraped 60 articles from'sports'.\n",
      "Scraping category 'science-technology'...\n",
      "\t--> Found 60 articles of 'science-technology'.\n",
      "\t--> Successfully scraped 60 articles from'science-technology'.\n",
      "Scraping category 'world'...\n",
      "\t--> Found 60 articles of 'world'.\n",
      "\t--> Successfully scraped 60 articles from'world'.\n",
      "Scraping category 'sports'...\n",
      "\t--> Found 101 articles in 'sports' category.\n",
      "\t--> Successfully scraped 99 articles from 'sports'.\n",
      "Scraping category 'business'...\n",
      "\t--> Found 100 articles in 'business' category.\n",
      "\t--> Successfully scraped 98 articles from 'business'.\n",
      "Scraping category 'entertainment'...\n",
      "\t--> Found 102 articles in 'entertainment' category.\n",
      "\t--> Successfully scraped 100 articles from 'entertainment'.\n",
      "Scraping category 'world'...\n",
      "\t--> Found 101 articles in 'world' category.\n",
      "\t--> Successfully scraped 99 articles from 'world'.\n",
      "Scraping category 'science-technology'...\n",
      "\t--> Found 99 articles in 'science-technology' category.\n",
      "\t--> Successfully scraped 97 articles from 'science-technology'.\n"
     ]
    }
   ],
   "source": [
    "express_df = scraper.get_express_articles()\n",
    "geo_df = scraper.get_geo_article()\n",
    "jang_df = scraper.get_jang_article()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 3 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to jang_articles.csv\n",
      "Data saved to geo_articles.csv\n",
      "Data saved to express_articles.csv\n",
      "Data saved to urdu_articles.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>'باہوبلی 2' کے اداکار 47 سال کی عمر میں شادی ک...</td>\n",
       "      <td>https://www.express.pk/story/2735199/bahubali2...</td>\n",
       "      <td>بالی ووڈ اور ساؤتھ انڈین فلم انڈسٹری کے معروف ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>نیٹ فلکس سیریز تنازع : دھنش نے نینتھارا کے خلا...</td>\n",
       "      <td>https://www.express.pk/story/2735191/netflixse...</td>\n",
       "      <td>تامل فلم انڈسٹری کے دو مشہور ستاروں، دھنش اور ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>گلیڈی ایٹر 2' نے باکس آفس پر تہلکہ مچایا، 106 ...</td>\n",
       "      <td>https://www.express.pk/story/2735188/gladiator...</td>\n",
       "      <td>ہالی وڈ کی دو بڑی فلموں، میوزیکل ایڈاپٹیشن \"وِ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>والد چنکی پانڈے طویل عرصے تک بےروزگار رہے، انن...</td>\n",
       "      <td>https://www.express.pk/story/2735173/chinkypan...</td>\n",
       "      <td>ممبئی - بالی وڈ اداکارہ اننیا پانڈے نے انکشاف ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>رشی کپور نے فلم ’برفی‘ پر بیٹے رنبیر کو کیا مش...</td>\n",
       "      <td>https://www.express.pk/story/2735167/rishikapo...</td>\n",
       "      <td>ممبئی - آنجہانی اداکار رشی کپور نے اپنی صاف گو...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  'باہوبلی 2' کے اداکار 47 سال کی عمر میں شادی ک...   \n",
       "1   1  نیٹ فلکس سیریز تنازع : دھنش نے نینتھارا کے خلا...   \n",
       "2   2  گلیڈی ایٹر 2' نے باکس آفس پر تہلکہ مچایا، 106 ...   \n",
       "3   3  والد چنکی پانڈے طویل عرصے تک بےروزگار رہے، انن...   \n",
       "4   4  رشی کپور نے فلم ’برفی‘ پر بیٹے رنبیر کو کیا مش...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.express.pk/story/2735199/bahubali2...   \n",
       "1  https://www.express.pk/story/2735191/netflixse...   \n",
       "2  https://www.express.pk/story/2735188/gladiator...   \n",
       "3  https://www.express.pk/story/2735173/chinkypan...   \n",
       "4  https://www.express.pk/story/2735167/rishikapo...   \n",
       "\n",
       "                                             content     gold_label  \n",
       "0  بالی ووڈ اور ساؤتھ انڈین فلم انڈسٹری کے معروف ...  entertainment  \n",
       "1  تامل فلم انڈسٹری کے دو مشہور ستاروں، دھنش اور ...  entertainment  \n",
       "2  ہالی وڈ کی دو بڑی فلموں، میوزیکل ایڈاپٹیشن \"وِ...  entertainment  \n",
       "3  ممبئی - بالی وڈ اداکارہ اننیا پانڈے نے انکشاف ...  entertainment  \n",
       "4  ممبئی - آنجہانی اداکار رشی کپور نے اپنی صاف گو...  entertainment  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "combined_df = pd.concat([express_df, geo_df,jang_df], axis=0, ignore_index=True)\n",
    "jang_df.to_csv('jang_articles.csv', index=False)\n",
    "print('Data saved to jang_articles.csv')\n",
    "geo_df.to_csv('geo_articles.csv', index=False)\n",
    "print('Data saved to geo_articles.csv')\n",
    "express_df.to_csv('express_articles.csv', index=False)\n",
    "print('Data saved to express_articles.csv')\n",
    "combined_df.to_csv('urdu_articles.csv', index=False)\n",
    "print('Data saved to urdu_articles.csv')\n",
    "\n",
    "combined_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
